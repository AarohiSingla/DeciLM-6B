{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2s2wIpT0AQq"
   },
   "source": [
    "# DeciLM-6B Tutorial -  Generating Text with a base LLM\n",
    "\n",
    "### Alright, let's dive into text generation with DeciLM-6B and the ü§ó HuggingFace `transformers` library! üöÄ\n",
    "\n",
    "LLMs, or Large Language Models, are the superheroes behind text generation.\n",
    "\n",
    "Imagine them as giant brains trained to predict the next word in a sequence.\n",
    "\n",
    "But, they don't just spit out words willy-nilly.\n",
    "\n",
    "They use a method called autoregressive generation, where they keep calling themselves with their own outputs until they've crafted a masterpiece (or at least a coherent sentence).\n",
    "\n",
    "### 2. Setting Up\n",
    "Before you start summoning words from the ether, make sure you've got the right tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bwLhaKVw8uue"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install huggingface_hub\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes>=0.39.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface version :  0.16.4\n",
      "transformers version :  4.34.0.dev0\n",
      "accelerate version :  0.22.0\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "print(\"huggingface version : \",huggingface_hub.__version__)\n",
    "print(\"transformers version : \", transformers.__version__)\n",
    "print(\"accelerate version : \", accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mS0SITJhDE_Y"
   },
   "source": [
    "Use the following token to log in:\n",
    "`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`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fClqIRBDCwyU"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7CHNHKX9ojt"
   },
   "outputs": [],
   "source": [
    "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KY6Xt76oBjYp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dioUBMkc0Up2"
   },
   "source": [
    "# üë©üèæ‚Äçüíª Let's Code!\n",
    "\n",
    "### a. Load the Model\n",
    "\n",
    "First, we need to download and summon our LLM.\n",
    "\n",
    "This could take a few minutes, depending on your internet connection. Grab a coffee or water, have a stretch, and check back in about one to three minutes.\n",
    "\n",
    "Here's the incantation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "65a39d9164df4ce2954617787b2ef75d",
      "251b24392393444aaa062d036e170683",
      "ead0da4006794e88b8693767ceeb2248",
      "f0d8f8382eeb46ae9ee27d9cc80a108e",
      "586ec18c7cb3465da4c84153cff1e594",
      "17d20ea6ad2d453e830b5e1c58e7befe",
      "207a87d065ac4780abe5d082ea2d3516",
      "a4f3cddb0f1e41269fa871a082c745f6",
      "aefaf9862fdd46e3b08faa3017a17abe",
      "54725beff64b4116891d54fed20c8b14",
      "91042f8416f24703b121651a9215559d"
     ]
    },
    "id": "6WNZ7wz_9pEd",
    "outputId": "cf3cd41c-747c-4bc9-ce90-5f1b84df2bf0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a39d9164df4ce2954617787b2ef75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On launch day the model will be\n",
    "# model_id = 'Deci/DeciLM-6B'\n",
    "\n",
    "model_id = 'Deci/test_decilm'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRdaXesV0ctE"
   },
   "source": [
    "### b. Preprocess with a Tokenizer\n",
    "\n",
    "Computers don't understand words in the way humans do. They prefer numbers. Tokenizers help bridge this gap by breaking down text and mapping each token to a unique number.\n",
    "\n",
    "So, before feeding our model, we need to translate our text into a language it understands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dW8VdC240dE0"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_inputs = tokenizer(\"A list of colors: red, blue\", return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UqbedDMES_a"
   },
   "source": [
    "### c. Generate Text\n",
    "\n",
    "Now, let the magic happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKPQMOuQwU5H",
    "outputId": "4e06d67c-9ea9-484a-f682-05e529735e03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs,\n",
    "                               max_new_tokens=40,\n",
    "                               num_beams=5,\n",
    "                               early_stopping=True\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh9UvcZc1ySa"
   },
   "source": [
    "After a model processes input text, it often returns a sequence of token IDs.\n",
    "\n",
    "These token IDs are numerical representations of the tokens (words, subwords, or characters) that the tokenizer originally produced.\n",
    "\n",
    "To make sense of these numbers and convert them back to human-readable text, we need to \"decode\" them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RR_Cctu1cUE"
   },
   "source": [
    "### d. Decode\n",
    "\n",
    "After processing, we often want to convert the tokens (or their machine-friendly number representations) back into human-readable text. This reverse process is called detokenization.\n",
    "\n",
    "`tokenizer.batch_decode` converts the numerical outputs of a model back into meaningful, human-readable sentences, making the results interpretable and usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSvBkP0jwVBc",
    "outputId": "8fbfcfed-7753-4e37-afc7-1fcedda9871f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list of colors: red, blue, green, yellow, orange, purple, pink, black, white, brown, gray, silver, and gold.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2idu3mFg-ZgS"
   },
   "source": [
    "# More examples\n",
    "\n",
    "Let's create a simple function which will take our prompt and number of new tokens to be generated and gives us the generated text.\n",
    "\n",
    "## How to control text generation\n",
    "\n",
    "Text generation with Hugging Face Transformers is both an art and a science.\n",
    "\n",
    "While model architecture and training data lay the foundation, parameters like `num_beams`, `no_repeat_ngram_size`, and `early_stopping` serve as fine-tuning knobs.\n",
    "\n",
    "By understanding and adeptly adjusting these parameters, you can significantly enhance the quality of your model's generated text.\n",
    "\n",
    "Experiment, iterate, and find the perfect balance for your unique application!\n",
    "\n",
    "I'll briefly describe how these three in particular influence the quality and characteristics of generated text:\n",
    "\n",
    "1) `num_beams`\n",
    "\n",
    "2) `no_repeat_ngram_size`\n",
    "\n",
    "3) `early_stopping`\n",
    "\n",
    "\n",
    "#### 1. üî¶ `num_beams`: The Power of Beam Search\n",
    "\n",
    "**What it does:** `num_beams` defines the number of sequences (or \"beams\") the model considers in parallel during generation. It essentially widens the search space, allowing the model to explore multiple possibilities before settling on the final sequence.\n",
    "\n",
    "**Effect on text:** A higher `num_beams` value often results in more coherent and contextually relevant sequences. However, it can also increase the computational overhead, as the model now has to track and update more sequences.\n",
    "\n",
    "**Practical tip:** If your model's output lacks fluency or seems off-context, consider increasing `num_beams`. Just be mindful of the trade-off between quality and computation time.\n",
    "\n",
    "#### 2. üôÖüèΩ `no_repeat_ngram_size`: Curbing Repetitiveness\n",
    "\n",
    "**What it does:** This parameter prevents the repetition of n-grams. An n-gram is a contiguous sequence of 'n' items from a text. Setting `no_repeat_ngram_size` to 2, for instance, ensures that the same set of two tokens doesn't appear more than once.\n",
    "\n",
    "**Effect on text:** It significantly reduces repetitiveness in the output. Especially in longer sequences, preventing repeated n-grams can make the output more readable and less redundant.\n",
    "\n",
    "**Practical tip:** If your generated text feels \"looped\" or tautological (ie, \"saying the same thing twice\"), tweaking `no_repeat_ngram_size` can be a game-changer.\n",
    "\n",
    "#### 3. üõë `early_stopping`: Knowing When to Stop\n",
    "\n",
    "**What it does:** `early_stopping` determines if the generation process should cease when all sequences in the beam reach the end-of-sequence token.\n",
    "\n",
    "**Effect on text:** By enabling `early_stopping`, you can ensure that the generation halts once a logical endpoint is reached, even if the `max_length` hasn't been attained. This often leads to more concise and relevant outputs.\n",
    "\n",
    "**Practical tip:** If your outputs seem unnecessarily lengthy or start drifting off-topic, enabling `early_stopping` can bring them back on track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMOSdOwBAjDG"
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt:str, max_new_tokens:int, temperature:float) -> str:\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**model_inputs,\n",
    "                                   max_new_tokens=max_new_tokens,\n",
    "                                   temperature=temperature,\n",
    "                                   num_beams=5,\n",
    "                                   no_repeat_ngram_size=4,\n",
    "                                   early_stopping=True\n",
    "                                   )\n",
    "    decoded_generation = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    return print(decoded_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UK8VcCO6wVXW",
    "outputId": "4d5135ec-dc2b-42ea-caf8-bbb7d4e45ea3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> In this blog post, we're going to talk about why waking up is one of the most important things you can do for your health and well-being. We're also going to give you some tips on how to make sure you're getting the most out of your mornings. So, if you're ready to start your day off on the right foot, let's get started!\n",
      "Why is waking up important?\n",
      "Waking up is important because it sets the tone for the rest of your day. If you wake up feeling groggy and unrested, it's going to be difficult to stay focused and productive throughout the day. On the other hand, waking up early and getting a good night's sleep can help you feel more energized and ready to take on the day.\n",
      "How can I make sure I'm waking up on time?\n",
      "If you're having trouble waking up in the morning, there are a few things you can try. One option is to set an alarm that goes off 15 minutes earlier than your usual wake-up time. This will give you a little bit of extra time to wake up and get ready for the day. Another option is to make your bedroom as dark as possible. This will help you fall asleep faster and wake up more easily in the morning.\n",
      "What are some tips for making the most of my mornings?\n",
      "There are a few tips you can follow to make the most of your morning routine. First, make sure you get enough sleep the night before. Second, wake up at the same time every day, even on weekends. Third, eat a healthy breakfast and drink plenty of water. Finally, take some time for yourself each morning to do something you enjoy, such as reading, meditating, or exercising. By following these tips, you'll be able to start each day feeling refreshed and ready to tackle whatever comes your way.\n",
      "How do I know if I'm getting enough sleep?\n",
      "There is no one-size-fits-all answer to this question, as everyone's sleep needs are different. However, as a general rule of thumb, most adults need between seven and nine hours of sleep per night. If you're not sure how much sleep you need, talk to your doctor or a sleep specialist. They can help you determine the right amount of sleep for you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"In this blog post, we're going to talk about why waking up is\"\"\"\n",
    "generate_text(prompt, 500, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4YZVZHyPS3W",
    "outputId": "353c1685-3dc5-4255-a2c7-7602aafc03e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Dear recruiter, I write this letter of recommendation for my toddler \n",
      "son for his application to the Hogwarts School of Monster Trucks and Classic Cars. \n",
      "He has over 100 monster trucks and this is beyond an obsession\n",
      "it's a way of life for him. \n",
      "His favorite truck is a 1969 Ford F-100 pickup truck with a 427 cubic inch \n",
      "V-8 engine and a 4-speed manual transmission. \n",
      "The truck has a 12-inch lift kit, 38-inch tires, and a 10-inch suspension lift. \n",
      "It also has a 500-cubic inch V-8 engine, a 6-speed automatic transmission, \n",
      "and a 3-inch exhaust system. \n",
      "This truck can go from 0 to 60 mph in less than 3 seconds and has a top speed \n",
      "of over 200 mph. \n",
      "My son loves this truck so much that he sleeps with it every night. \n",
      "When he wakes up in the morning, the first thing he does is check to see \n",
      "if the truck is still there. \n",
      "If it isn't, he gets really upset and starts crying. \n",
      "I've tried to explain to him that the truck isn't real, but he doesn't \n",
      "believe me. \n",
      "Even when I show him pictures of the truck on the internet, he still \n",
      "thinks it's real. \n",
      "So, I'm writing this letter to let you know that my son is a very \n",
      "intelligent and talented young man who has a great passion for monster \n",
      "trucks and classic cars. \n",
      "Thank you for your time and consideration. \n",
      "Sincerely, \n",
      "Mom\n",
      "\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Dear recruiter, I write this letter of recommendation for my toddler\n",
    "son for his application to the Hogwarts School of Monster Trucks and Classic Cars.\n",
    "He has over 100 monster trucks and this is beyond an obsession\n",
    "\"\"\"\n",
    "generate_text(prompt, 500, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRxgoyTXCVFa",
    "outputId": "e87d37f0-6de5-492f-d263-c207f6109356"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> It was a clear dark night, a clear white moon. Warren G was on the street trying to consume as much alcohol as he possibly could. He was on his way to a party, but he wasn't really looking forward to it. He just wanted to get drunk and forget about his problems.\n",
      "Warren had a lot of problems. His girlfriend was cheating on him, his boss was a jerk, and he didn't know what he was going to do with his life. He was 25 years old, and he had no idea what he wanted to do with the rest of his life.\n",
      "He was walking down the street when he saw a man standing on the corner. The man was holding a sign that said, \"Will work for food.\" Warren was hungry, so he stopped to talk to the man.\n",
      "The man told Warren that he was homeless, and that he had been living on the street for the past few months. He said that he was looking for a job, but that no one would hire him. Warren felt sorry for the man, and he offered to buy him a meal.\n",
      "The two of them went to a nearby restaurant, and Warren ordered a meal for the man. After they ate, Warren asked the man what his name was. The man told him that his name was John. Warren asked John if he would like to go to the party with him, and John said that he would love to.\n",
      "John and Warren had a great time at the party. They drank a lot of alcohol, and Warren even danced with a few girls. When the party was over, John and Warren went back to Warren's apartment. Warren offered John a place to stay for the night, and John accepted.\n",
      "The next morning, Warren woke up and found John asleep in his bed. He was surprised to see John there, but he was also happy that he had someone to talk to. Warren and John had a long conversation about their lives, and they realized that they had a lot in common.\n",
      "They decided to become best friends, and they have been friends ever since. Warren has helped John get a job, and John has helped Warren with his problems. They have been through a lot together, and they will always be there for each other.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"It was a clear dark night, a clear white moon. Warren G was on the street trying to consume\"\"\"\n",
    "generate_text(prompt, 500, 0.7)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "17d20ea6ad2d453e830b5e1c58e7befe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "207a87d065ac4780abe5d082ea2d3516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "251b24392393444aaa062d036e170683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17d20ea6ad2d453e830b5e1c58e7befe",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_207a87d065ac4780abe5d082ea2d3516",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "54725beff64b4116891d54fed20c8b14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "586ec18c7cb3465da4c84153cff1e594": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65a39d9164df4ce2954617787b2ef75d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_251b24392393444aaa062d036e170683",
       "IPY_MODEL_ead0da4006794e88b8693767ceeb2248",
       "IPY_MODEL_f0d8f8382eeb46ae9ee27d9cc80a108e"
      ],
      "layout": "IPY_MODEL_586ec18c7cb3465da4c84153cff1e594"
     }
    },
    "91042f8416f24703b121651a9215559d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4f3cddb0f1e41269fa871a082c745f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aefaf9862fdd46e3b08faa3017a17abe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ead0da4006794e88b8693767ceeb2248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4f3cddb0f1e41269fa871a082c745f6",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aefaf9862fdd46e3b08faa3017a17abe",
      "value": 3
     }
    },
    "f0d8f8382eeb46ae9ee27d9cc80a108e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54725beff64b4116891d54fed20c8b14",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_91042f8416f24703b121651a9215559d",
      "value": " 3/3 [00:09&lt;00:00,  2.96s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
